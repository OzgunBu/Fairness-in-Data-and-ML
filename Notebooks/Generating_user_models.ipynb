{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 0.19.1\n",
      "pandas: 0.23.0\n",
      "kerads: 2.1.6\n"
     ]
    }
   ],
   "source": [
    "# HIDE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True, context=\"talk\")\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import keras as ke\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "create_gif = False\n",
    "\n",
    "# model reconstruction from JSON:\n",
    "from keras.models import model_from_json\n",
    "\n",
    "print(f\"sklearn: {sk.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"kerads: {ke.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating_user_models.ipynb    nn3-arch.txt\r\n",
      "\u001b[34mObtaining Datasets\u001b[m\u001b[m/             \u001b[31mproduct1_main.py\u001b[m\u001b[m*\r\n",
      "\u001b[31mREADME.md\u001b[m\u001b[m*                      step_by_step-10.ipynb\r\n",
      "\u001b[34mTrade-off-results\u001b[m\u001b[m/              step_by_step-11.ipynb\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m/                    step_by_step-12.ipynb\r\n",
      "\u001b[34mdata\u001b[m\u001b[m/                           step_by_step-7.ipynb\r\n",
      "main_task_ori_trained_model.h5  step_by_step-8.ipynb\r\n",
      "my_functions_product1.py        step_by_step-9.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in your data are:\n",
      "  ['39' ' State-gov' ' 77516' ' Bachelors' ' 13' ' Never-married'\n",
      " ' Adm-clerical' ' Not-in-family' ' White' ' Male' ' 2174' ' 0' ' 40'\n",
      " ' United-States' ' <=50K']\n",
      "Your possible target columns '{' 13', ' Never-married', ' Bachelors', ' White', ' 40', ' 77516', ' United-States', ' 2174', '39', ' <=50K', ' Male', ' Adm-clerical', ' 0', ' Not-in-family', ' State-gov'}'\n",
      "Which column is your target column?: <=50K\n",
      "Your target column is ' <=50K'\n",
      "-----------------------------------\n",
      "\n",
      "Do you have any column for your current prediction? Please enter Y or N:N\n",
      "Do you have any column for your current prediction? You entered'N'\n",
      "-----------------------------------\n",
      "\n",
      "Your prediction column possible values '{' >50K', ' <=50K'}'\n",
      "Please enter two different labels in the target field: \n",
      "Enter label 0: <=50K\n",
      "Your target label 0 is ' <=50K'\n",
      "-----------------------------------\n",
      "\n",
      "Enter label 1: >50K\n",
      "Your target label 1 is ' >50K'\n",
      "-----------------------------------\n",
      "\n",
      "Your possible Sensitive Attribute columns '{' 13', ' Never-married', ' Bachelors', ' White', ' 40', ' 77516', ' United-States', ' 2174', '39', ' Male', ' Adm-clerical', ' 0', ' Not-in-family', ' State-gov'}'\n",
      "Which column is your sensitive column?: Male\n",
      "Your sensitive attribute column is ' Male'\n",
      "-----------------------------------\n",
      "\n",
      "unique values from the sensitive class:{' Female', ' Male'}\n",
      "Please enter two different classes in the sensisitve field: \n",
      "Enter class 0: Male\n",
      "Your sensitive attribute class 0 is ' Male'\n",
      "-----------------------------------\n",
      "\n",
      "Enter class 1: Female\n",
      "Your sensitive attribute class 1 is ' Female'\n",
      "-----------------------------------\n",
      "\n",
      "Do you want to omit any column from your data? Please enter Y or N:N\n",
      "Do you have any column for your current prediction? You entered'N'\n",
      "-----------------------------------\n",
      "\n",
      "All columns are kept.\n",
      "target field: <=50K\n",
      "target field: <=50K Label 0-1  <=50K,  >50K\n",
      "sensitive field: Male Class 0-1  Male,  Female\n",
      "not to consider fields []\n",
      "result file:  data--adult.data_CP_N_T__<=50K__<=50K__>50K_S__Male__Male__Female_\n",
      "Target field: p_rule_for_Y1 35.80\n"
     ]
    }
   ],
   "source": [
    "import my_functions_product1 as myFC\n",
    "\n",
    "# reading the file from a path and list the field names and input columns of interest\n",
    "path = 'data/adult.data'\n",
    "#path = 'data/compas.csv'\n",
    "#path = 'data/bank.csv'\n",
    "X_df, Ybin, Zbin, result_fname = myFC.read_process_data_output_bias(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (32560, 100)\n",
      "<class 'pandas.core.frame.DataFrame'> (32560, 14)\n"
     ]
    }
   ],
   "source": [
    "X = pd.get_dummies(X_df,drop_first=True) \n",
    "print(type(X),X.shape)\n",
    "print(type(X_df),X_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16280, 100)\n",
      "(16280, 100)\n",
      "(16280, 100)\n",
      "(16280, 100)\n"
     ]
    }
   ],
   "source": [
    "#X = pd.get_dummies(X_df,drop_first=True) \n",
    "Y = Ybin \n",
    "Z = Zbin\n",
    "test_train_ratio = 0.5\n",
    "\n",
    "#TODO: Can I do this in Keras?\n",
    "# TODO : what should be left to the user\n",
    "\n",
    "# split into train/test set\n",
    "X_train, X_test, y_train, y_test, Z_train, Z_test = train_test_split(X, Y, Z, test_size=test_train_ratio, \n",
    "                                                                     stratify=Y, random_state=7)\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
    "X_train = X_train.pipe(scale_df, scaler) \n",
    "X_test = X_test.pipe(scale_df, scaler) \n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#X_train = np.array(X_train)\n",
    "#X_test = np.array(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving scaled features: ready to be used for training:\n",
    "data = X_train\n",
    "ex_str = 'X_train.csv'\n",
    "csv_name = result_fname+'X_train.csv'\n",
    "data.to_csv(csv_name, index=False)\n",
    "\n",
    "data = X_test\n",
    "ex_str = 'X_test.csv'\n",
    "csv_name = result_fname+'X_test.csv'\n",
    "data.to_csv(csv_name, index=False)\n",
    "\n",
    "data = y_train\n",
    "ex_str = 'y_train.csv'\n",
    "csv_name = result_fname+'y_train.csv'\n",
    "data.to_csv(csv_name, index=False)\n",
    "\n",
    "data = y_test\n",
    "ex_str = 'y_test.csv'\n",
    "csv_name = result_fname+'y_test.csv'\n",
    "data.to_csv(csv_name, index=False)\n",
    "\n",
    "data = Z_train\n",
    "ex_str = 'Z_train.csv'\n",
    "csv_name = result_fname+'Z_train.csv'\n",
    "data.to_csv(csv_name, index=False)\n",
    "\n",
    "data = Z_test\n",
    "ex_str = 'Z_test.csv'\n",
    "csv_name = result_fname+'Z_test.csv'\n",
    "data.to_csv(csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mObtaining Datasets\u001b[m\u001b[m/             \u001b[31mproduct1_main.py\u001b[m\u001b[m*\r\n",
      "\u001b[31mREADME.md\u001b[m\u001b[m*                      step_by_step-10.ipynb\r\n",
      "\u001b[34mTrade-off-results\u001b[m\u001b[m/              step_by_step-11.ipynb\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m/                    step_by_step-12.ipynb\r\n",
      "\u001b[34mdata\u001b[m\u001b[m/                           step_by_step-7.ipynb\r\n",
      "main_task_ori_trained_model.h5  step_by_step-8.ipynb\r\n",
      "my_functions_product1.py        step_by_step-9.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier_arch(n_features):\n",
    "    inputs = Input(shape=(n_features,))\n",
    "    dense1 = Dense(32, activation='relu')(inputs)\n",
    "    dropout1 = Dropout(0.2)(dense1)\n",
    "    dense2 = Dense(32, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.2)(dense2)\n",
    "    dense3 = Dense(32, activation=\"relu\")(dropout2)\n",
    "    dropout3 = Dropout(0.2)(dense3)\n",
    "    dense4 = Dense(32, activation=\"relu\")(dropout3)\n",
    "    dropout4 = Dropout(0.2)(dense4)\n",
    "    outputs = Dense(1, activation='sigmoid')(dropout4)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adversary_arch(inputs):\n",
    "    dense1 = Dense(32, activation='relu')(inputs)\n",
    "    dense2 = Dense(32, activation='relu')(dense1)\n",
    "    dense3 = Dense(32, activation='relu')(dense2)\n",
    "    dense4 = Dense(32, activation=\"relu\")(dense3)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense4)# for _ in range(n_sensitive)]\n",
    "    return Model(inputs=[inputs], outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and saving the classifer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_task_arch = Classifier_arch(n_features=X_train.shape[1])\n",
    "main_task_arch_json_string = main_task_arch.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and saving the  adversary architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_inputs = Input(shape=(1,))\n",
    "adv_task_arch = Adversary_arch(adv_inputs)\n",
    "adv_task_arch_json_string = adv_task_arch.to_json()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try reading the classifier architecture and model/compile and check prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_task_ori = model_from_json(main_task_arch_json_string)\n",
    "# initialise NeuralNet Classifier\n",
    "main_task_ori.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "# train on train set\n",
    "main_task_ori.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "\n",
    "main_task_accuracy, p_rule_for_Y1, y_pred = bias_accuracy_performance(X_test,y_test,Z_test,main_task_ori)\n",
    "main_task_ori.save_weights('main_task_ori_trained_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "del main_task_ori  # deletes the existing model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the main task arch with the adversarial arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair model - training adversarial and classifier together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tradeoff_lambda_v = [0, 10, 20, 30, 60, 100] \n",
    "pre_load_flag = True\n",
    "\n",
    "main_task_trained_weight_file = 'main_task_ori_trained_model.h5'\n",
    "\n",
    "for tradeoff_lambda in tradeoff_lambda_v:\n",
    "    print('tradeoff_lambda = ', tradeoff_lambda)\n",
    "    # initialise FairClassifier\n",
    "    clf = myFC.FairClassifier(tradeoff_lambda=tradeoff_lambda,\n",
    "                     main_task_arch_json_string=main_task_arch_json_string,\n",
    "                     adv_task_arch_json_string=adv_task_arch_json_string,\n",
    "                     pre_load_flag=pre_load_flag,main_task_trained_weight_file=main_task_trained_weight_file)\n",
    "    \n",
    "    # pre-train both adverserial and classifier networks\n",
    "    clf.pretrain(X_train, y_train, Z_train, verbose=0, epochs=5,pre_load_flag=pre_load_flag,main_task_trained_weight_file=main_task_trained_weight_file)\n",
    "    before_main_task_accuracy, before_p_rule_for_Y1,before_y_pred = bias_accuracy_performance(X_test,y_test,Z_test,clf)\n",
    "    \n",
    "    # adverserial train on train set and validate on test set\n",
    "    clf.fit(X_train, y_train, Z_train, \n",
    "            validation_data=(X_test, y_test, Z_test),\n",
    "            T_iter=165, save_figs=create_gif)\n",
    "    after_main_task_accuracy, after_p_rule_for_Y1,after_y_pred = bias_accuracy_performance(X_test,y_test,Z_test,clf)\n",
    "    \n",
    "    \n",
    "    result_fname_y_pred_before_after,result_fname_acc_p_before_after = saving_performance_result(before_main_task_accuracy, before_p_rule_for_Y1,before_y_pred, after_main_task_accuracy, after_p_rule_for_Y1,after_y_pred,tradeoff_lambda,result_fname)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting the results; trade off curve and initial and final distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeoff_lambda_v = [0, 10, 20, 30, 60, 100] \n",
    "\n",
    "Bacc_Bp_Aacc_Ap_results = np.zeros([len(tradeoff_lambda_v),4])\n",
    "\n",
    "Bacc_Bp_Aacc_Ap_results = np.zeros([len(tradeoff_lambda_v),4])\n",
    "\n",
    "for item in range(len(tradeoff_lambda_v)):\n",
    "    \n",
    "    tradeoff_lambda = tradeoff_lambda_v[item]\n",
    "    result_fname_acc_p_before_after = 'acc_p_BA_'+result_fname + 'L'+ str(tradeoff_lambda)+'.txt'\n",
    "    result_fname_acc_p_before_after = './Trade-off-results/'+result_fname_acc_p_before_after\n",
    "    Bacc_Bp_Aacc_Ap = np.loadtxt(result_fname_acc_p_before_after, delimiter=',')\n",
    "\n",
    "    Bacc_Bp_Aacc_Ap_results[item, :] = Bacc_Bp_Aacc_Ap\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(Bacc_Bp_Aacc_Ap_results[:,3], 100*Bacc_Bp_Aacc_Ap_results[:,2])\n",
    "plt.xlabel('p-score')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Trade off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fname_y_pred_before_after = 'y_pred_'+result_fname +'L' + str(tradeoff_lambda) +'.txt'\n",
    "result_fname_y_pred_before_after = './Trade-off-results/'+ result_fname_y_pred_before_after\n",
    "\n",
    "BA_y_pred = np.loadtxt(result_fname_y_pred_before_after, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before the de-biaser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(BA_y_pred[Z_test == 0,0], hist=False, \n",
    "                  kde_kws={'shade': True,},\n",
    "                  label='{}'.format('0'))\n",
    "\n",
    "ax = sns.distplot(BA_y_pred[Z_test == 1,0], hist=False, \n",
    "                  kde_kws={'shade': True,},\n",
    "                  label='{}'.format('1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the de-biaser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(BA_y_pred[Z_test == 0,1], hist=False, \n",
    "                  kde_kws={'shade': True,},\n",
    "                  label='{}'.format('0'))\n",
    "\n",
    "ax = sns.distplot(BA_y_pred[Z_test == 1,1], hist=False, \n",
    "                  kde_kws={'shade': True,},\n",
    "                  label='{}'.format('1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
